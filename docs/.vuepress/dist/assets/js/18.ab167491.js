(window.webpackJsonp=window.webpackJsonp||[]).push([[18],{448:function(e,t,n){"use strict";n.r(t);var a=n(16),o=Object(a.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"learning-feedforward-neural-network-through-xor"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#learning-feedforward-neural-network-through-xor"}},[e._v("#")]),e._v(" Learning Feedforward Neural Network Through XOR")]),e._v(" "),n("p",[e._v("To getting more clear about Feedforward Nerual Network, we work on a sample XOR example. In this case, we are going to find a function $$f(x; \\theta)$$ to match the true model $f^*$.")]),e._v(" "),n("p",[e._v("In this simple example, we will not be concerned with statistical generalization.")]),e._v(" "),n("p",[e._v("First, we treat this problem as a linear regression problem, and using Mean squared error (MSE) lose function. Assume our model is $f(x;w,b)=x^\\top w+b$. We could use normal equation to minimalize lost. In this case, we got $w=0$ and $b=\\frac{1}{2}$ when the lost is minimal, and the output of this mode would be $\\frac{1}{2}$ all the time. We know the output is not correct. We want to get 1 when two inputs are different, we want to get 0 when two inputs are the same.  The pictures below shown why linear model cannot represent XOR problem.")]),e._v(" "),n("p",[e._v("If we introduce a simple FNN with one hidden layer, the problem could be solved.The latest model is $f(x;W,c,w,b)=f^{(2)}(f^{(1)(x)})$ where $h=f^{(1)}(x;W,c)$ and $y=f^{(2)}(h;w,b)$. Due to $f^{(2)}$ is a linear function, then $f^{(1)}$ cannot be linear because the output would be linear. And we know linear output is incorrect. Clearly, we must use a nonlinear function to describe the features. Most neural networks do so using an aﬃne transformation controlled by learned parameters, followed by a ﬁxed nonlinear function called an activation function. We use that strategy here, by deﬁning $\\mathbf{h}=g(\\mathbf{W}^\\top+c)$, where $\\mathbf{W}$ provides the weights of a linear transformation and $c$ is biases.")]),e._v(" "),n("p",[e._v("Previously, to describe a linear regression model, we used a vector of weights and a scalar bias parameter to describe an aﬃne transformation from an input vector to an output scalar. Now, we describe $\\mathbf{h}$ an aﬃne transformation from a vector $\\mathbf{x}$ to a vector $\\mathbf{h}$, so an entire vector of bias parameters is needed. The activation function $g$ is typically chosen to be a function that is applied element-wise, with $$h_i=g(x^\\top\\mathbf{W}_{:,i}+c_i)$$. In modern neural networks, the default recommendation is to use the "),n("strong",[e._v("rectified linear unit")]),e._v(", or ReLU, defined by the activation function $g(z) = max{0, z}$.")]),e._v(" "),n("p",[n("img",{attrs:{src:"https://github.com/jinchenxiangdan/myBlog/blob/master/docs/.vuepress/public/images/xor-prob.png?raw=true",alt:"XOR prob"}})]),e._v(" "),n("p",[e._v("We can now specify our complete network as")]),e._v(" "),n("p",[e._v("$$f(x;W,c,w,b)=w^\\top max{0, W^\\top x+c}+b$$")]),e._v(" "),n("p",[e._v("And we can then specify a solution to the XOR problem, let\n$$\nW=\n\\left[\n\\begin{matrix}\n1 & 1  \\\n1 & 1\n\\end{matrix}\n\\right]\n$$")]),e._v(" "),n("p",[e._v("$$\nc=\n\\left[\n\\begin{matrix}\n0  \\\n-1\n\\end{matrix}\n\\right]\n$$")]),e._v(" "),n("p",[e._v("$$\nw=\n\\left[\n\\begin{matrix}\n1  \\\n-2\n\\end{matrix}\n\\right]\n$$")]),e._v(" "),n("p",[e._v("and $b = 0$.")])])}),[],!1,null,null,null);t.default=o.exports}}]);